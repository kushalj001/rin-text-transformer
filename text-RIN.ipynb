{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1523a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d2756-9b72-4164-afc0-34ba7565746b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12702ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORK LOG ###\n",
    "\n",
    "# this notebook is to try out an RIN arch for text.\n",
    "# plan is to begin by adapting the main layers and training code from\n",
    "# lucid rain's existing rin codebase for images. There are 2 things, in major\n",
    "# to change: one is the embeddings at the interface input and the second is \n",
    "# tokenization. Tokenization can be tinkered with later on. We first\n",
    "# begin by just trying out token embeddings instead of patch embeddings at\n",
    "# interface input.\n",
    "\n",
    "# the way the arch works is that we give token embeddings corrupted with\n",
    "# gaussian noise as input, and pass it through a series of read-compute-write\n",
    "# blocks and ultimately try to reconstruct the uncorrupted input.\n",
    "\n",
    "# using sinusoidal embeddings for timesteps from Diffusion-LM. RIN has a module\n",
    "# for the same too, but I believe it would be better to go with something that\n",
    "# has worked with language before. They seem to be doing similar things, regardless.\n",
    "# I have ported RIN one too, just in case.\n",
    "\n",
    "# RIN uses a custom LN module without bias, saying it is more stable. \n",
    "# I am going to use the standard one from torch.\n",
    "\n",
    "## NOTE ON PATCH SIZE AND NUMBER OF PATCHES\n",
    "# (b, c, h, w) = (b, 3, 128, 128) => (b, (16*16), (3*8*8)) = [b, 256, 192]\n",
    "# imagine an image with 3 channels, and each channel is an image of 128X128. \n",
    "# For each channel image, we want to divide it into 256 patches, each of size 8X8.\n",
    "# You can also think about creating patches of the entire image, with the channels all \n",
    "# combined. \n",
    "# So we technically get 256 patches in total, each of size 8X8X3. So each patch is\n",
    "# an embedding of dim 192.\n",
    "\n",
    "### RIN block code flow\n",
    "# input: patches=[bs, num_patches, dim], latents=[bs, num_latents, latent_dim], time_embedding=[bs, time_dim]\n",
    "# num_patches = patch_h X patch_w\n",
    "# In terms of its similarity with tensors for text sequences, you can think \n",
    "# of the 2nd dimesnion being seq_len for both patches and latents (which it actually is).\n",
    "# The 2nd dim in patches is ph*pw => num_patches, and num_latents in latents.\n",
    "# So the transformation from patches to latents involves compressing information\n",
    "# along 2 axes, one across the sequence length (number of tokens) and the second \n",
    "# across the dimensionality of each token embedding. In this running example, we're only\n",
    "# compressing across the first dimension and expanding across the second dimension\n",
    "# (which kind of does not make sense lol). \n",
    "# 1. apply PEG layer (performs a 2d conv on the patches). No shape changes overall and would be irrelevant for text.\n",
    "# 2. apply cross attention to project patches to latent space. Layer name is `latent_attends_to_patches`. \n",
    "# 3. apply ffn on cross attn output in latent space\n",
    "# Note: in both 2 and 3, time_embedding is also passed. It's handled in different ways (not going into detail here).\n",
    "# 4. loop in `latent_self_attns` which is a nn module list of Attention and FF layers.\n",
    "#    apply attn and ff for each layer and add the residuals\n",
    "# 5. an additional layer of self attention is applied. The layer is different (LinearAttention). Check.\n",
    "# 6. apply cross attention from latent space to patches\n",
    "# 7. apply ff in patch space, apply norm and return the final tensor.\n",
    "\n",
    "## ATTENTION LAYER CODE FLOW (cross attention) ##\n",
    "# input: latents = [bs, num_latents, latent_dim], patches = [bs, num_patches, dim], time = [bs, time_dim]\n",
    "# 1. apply LN on patches\n",
    "# 2. apply LN on latents\n",
    "# 3. apply a sequential module on time input which consists of SiLU => Linear() => Rearrange(). \n",
    "#    The linear layer does not change the dimension of the vector. Rearrange adds an additional dimension at 1st index position\n",
    "#    making the shape [bs, 1, time_dim]. The output of this layer is chunked across the last dimension into 2 tensors of\n",
    "#    [bs, 1, time_dim//2]. The 2 tensors are called scale and shift. Not sure what they represent.\n",
    "# 4. applied to latents: x = x*scale + shift.\n",
    "# 5. 2 linear layers are defined to convert the inputs to relevant query and key values. For cross attention, latents are used\n",
    "#    as queries and the patches are used as key and values. The projection of patches to kv is fused. (not sure how different this \n",
    "#    is than normal way of doing this projection separately.). First get q values by mapping latents and then kv. Dimensions of q,k,v are\n",
    "#    head_dim*num_heads=model_dim. \n",
    "#    This operation gives q of shape [bs, num_latents, model_dim] and kv of shape [bs, num_patches, model_dim].\n",
    "# 6. Reshape it to [bs, num_heads, num_latents (or num_patches), head_dim]. num_patches = num_latents.\n",
    "# 7. In attend func: first mask if necessary (check since does not seem to happen in the image codebase).\n",
    "# 8. Calculate similarity by multiplying q and k. Result is [bs, heads, num_latents, num_patches].\n",
    "# 9. Multiply it by scale (model_dim ** -0.5).\n",
    "# 10. Take softmax across last dim + dropout.\n",
    "# 11. Multiply by v of shape [bs, heads, num_patches, head_dim] to get => [bs, heads, num_latents, head_dim].\n",
    "# 12. Finally dim out to project it back to latent dim.\n",
    "\n",
    "\n",
    "### NOTE ON ATTENTION vs LINEARATTENTION ###\n",
    "# LinearAttention module is an efficient version of quadratic attention where k and v are multiplied first and the result is\n",
    "# multiplied with q. Softmax is applied before taking products at relevant places. The overall result stands because it can be derived\n",
    "# as a low rank approximation for the quadratic attention. \n",
    "# I can port it for now, but initially we can choose to just use the normal quadratic attention. I am also not sure how helpful linear\n",
    "# attention is when applied to latents. As it is latents are the compressed version of the actual interface tokens. Per my understanding, it\n",
    "# would make more sense to use it for longer sequences. Perhaps its more common in image datasets with high resolution data points.\n",
    "\n",
    "# TODO: read more about `latent_token_time_cond` flag, what it signifies and the literature around it.\n",
    "# TODO: understand what learned sinusoidal pos embedding does.\n",
    "\n",
    "\n",
    "### FFN CODE FLOW ###\n",
    "# 1. Apply LN on input x.\n",
    "# 2. If time_cond_dim is passed in the initialization, add that information to the input by applying a net with SiLU() + Linear() + Rearrange().\n",
    "#    The output is chunked across the last dim to get scale and shift. Then x = x * scale + shift. \n",
    "# 3. Linear() + GeLU() + Linear() is applied where first linear projects it to dim*4, and the final linear projects it back to dim.\n",
    "\n",
    "\n",
    "### PLANNING ###\n",
    "# (read/understand individual modules and adapt them for text)\n",
    "# Read Attention and LinearAttention modules, figure out which is needed. \n",
    "# Rewrite the classes with more favorable naming.\n",
    "# Rewrite FFN.\n",
    "# Note how LN is used in text models (diffusion-lm) and use it in that way over here instead of using the image conventions.\n",
    "# Rewrite RINBlock and stitch all the previous layers\n",
    "# Rewrite RIN with previous layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996391ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    # timesteps = [bs]\n",
    "    # dim = 128\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    # 64\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "    ).to(device=timesteps.device)\n",
    "    # exp{-(T * t)/64}, where T is the time period, t is the current time instant\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        # TODO: why cating zeros?\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da929e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = timestep_embedding(torch.arange(32), dim=128)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59bbc825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x281dba953f0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(emb[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de819fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedSinusoidalPosEmb(nn.Module): # used in RIN (image modality)\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        assert (dim % 2) == 0\n",
    "        half_dim = dim // 2\n",
    "        self.weights = nn.Parameter(torch.randn(half_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b -> b 1')\n",
    "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
    "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
    "        fouriered = torch.cat((x, fouriered), dim = -1)\n",
    "        return fouriered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39a99ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = LearnedSinusoidalPosEmb(128)\n",
    "# plt.plot(emb.forward(torch.arange(32))[1].tolist())\n",
    "\n",
    "# the embedding graph does not look as structured as the previous most likely\n",
    "# because this is a learnable embedding unlike the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ffad82d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim:int, # the expected output dimensionality after cross attention (query dimensionality)\n",
    "        context_dim:int, # dimensionality of the current stream of computation (kv dimensionality)\n",
    "        num_heads:int,\n",
    "        head_dim:int,\n",
    "        time_dim:int,\n",
    "        attn_dropout:float,\n",
    "        input_norm:bool,\n",
    "        context_norm:bool,\n",
    "        qk_norm:bool\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        attn_dim = head_dim * num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.num_heads = num_heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "        \n",
    "        if context_dim is None:\n",
    "            context_dim = output_dim\n",
    "        \n",
    "        if time_dim is not None:\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(time_dim, output_dim*2),\n",
    "                Rearrange('b d -> b 1 d')\n",
    "            )\n",
    "\n",
    "            # TODO: does this make sense for text?\n",
    "            nn.init.zeros_(self.time_mlp[-2].weight)\n",
    "            nn.init.zeros_(self.time_mlp[-2].bias)\n",
    "\n",
    "        # TODO: needed for text?\n",
    "        self.input_ln = nn.LayerNorm(output_dim) if input_norm else nn.Identity()\n",
    "        self.context_ln = nn.LayerNorm(context_dim) if context_norm else nn.Identity()\n",
    "\n",
    "        self.to_q = nn.Linear(output_dim, attn_dim, bias=False)\n",
    "        # TODO: how is this different than separately projecting k and v\n",
    "        self.to_kv = nn.Linear(context_dim, attn_dim*2, bias=False)\n",
    "        self.to_out = nn.Linear(attn_dim, output_dim, bias=False)\n",
    "\n",
    "    def attend(self, q, k, v, mask=None):\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        if mask is not None: # TODO: check if we need masking\n",
    "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
    "\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        attn = F.dropout(attn, p=self.attn_dropout)\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inp,\n",
    "        context,\n",
    "        time\n",
    "    ):\n",
    "        h = self.num_heads\n",
    "        if context is not None:\n",
    "            context = self.context_ln(context)\n",
    "\n",
    "        inp = self.input_ln(inp)\n",
    "        if context is None:\n",
    "            context = inp\n",
    "\n",
    "        scale, shift = self.time_mlp(time).chunk(2, dim=-1)\n",
    "        inp = (inp * (scale + 1)) + shift\n",
    "\n",
    "        qkv = (self.to_q(inp), *self.to_kv(context).chunk(2, dim=-1))\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "\n",
    "        out = self.attend(q, k, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "215eef95-9522-4357-8069-2e71c8afea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attention_module():\n",
    "    attn = Attention(512, 256, 4, 32, 1024, 0.2, True, True, True)\n",
    "    \n",
    "    inp = torch.randn(32, 128, 512)\n",
    "    context = torch.randn(32, 256, 256)\n",
    "    time = torch.randn(32, 1024)\n",
    "    attn_out = attn(inp, context, time)\n",
    "    assert attn_out.shape == torch.randn(32, 128, 512).shape\n",
    "\n",
    "test_attention_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41134acc-92a7-4e9c-8b1c-80f368f26b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim:int,\n",
    "        time_dim:int,\n",
    "        mult_factor:int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        if time_dim is not None:\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(time_dim, input_dim*2),\n",
    "                Rearrange(\"b d -> b 1 d\")\n",
    "            )\n",
    "            nn.init.zeros_(self.time_mlp[-2].weight)\n",
    "            nn.init.zeros_(self.time_mlp[-2].bias)\n",
    "\n",
    "        inner_dim = input_dim*mult_factor\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(input_dim, inner_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(inner_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, time=None):\n",
    "\n",
    "        x = self.norm(x)\n",
    "        if time is not None:\n",
    "            scale, shift = self.time_mlp(time).chunk(2, dim=-1)\n",
    "            x = (x*(scale+1)) + shift\n",
    "\n",
    "        return self.ffn(x)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34f555b4-e383-4061-b67d-466c99ba8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ffn_module():\n",
    "    ffn = FeedForwardLayer(256, 1024, 4)\n",
    "    inp = torch.randn(32, 128, 256)\n",
    "    time = torch.randn(32, 1024)\n",
    "    ffn_out = ffn(inp, time)\n",
    "    assert ffn_out.shape == torch.randn(32, 128, 256).shape\n",
    "\n",
    "test_ffn_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92f8ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RINBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        interface_dim:int,\n",
    "        num_latent_attn_layers:int,\n",
    "        latent_dim:int,\n",
    "        time_dim:int,\n",
    "        final_norm:bool\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        attn_kwargs = {\n",
    "            \"num_heads\": config.num_heads,\n",
    "            \"head_dim\": config.head_dim,\n",
    "            \"time_dim\": config.time_dim,\n",
    "            \"attn_dropout\": config.attn_dropout,\n",
    "            \"input_norm\": config.input_norm,\n",
    "            \"context_norm\": config.context_norm,\n",
    "            \"qk_norm\": config.qk_norm\n",
    "        }\n",
    "        self.interface_to_latents_cross_attn = Attention(\n",
    "            output_dim=latent_dim,\n",
    "            context_dim=interface_dim,\n",
    "            **attn_kwargs\n",
    "        )\n",
    "\n",
    "        self.latents_ffn = FeedForwardLayer(\n",
    "            input_dim=latent_dim,\n",
    "            time_dim=time_dim,\n",
    "            mult_factor=config.mult_factor\n",
    "        )\n",
    "\n",
    "        self.latent_self_attn_layers = nn.ModuleList([])\n",
    "        for _ in range(num_latent_attn_layers):\n",
    "            self.latent_self_attn_layers.append(\n",
    "                nn.ModuleList([\n",
    "                    Attention(\n",
    "                        output_dim=latent_dim,\n",
    "                        context_dim=None,\n",
    "                        **attn_kwargs\n",
    "                    ),\n",
    "                    FeedForwardLayer(\n",
    "                        input_dim=latent_dim,\n",
    "                        time_dim=time_dim,\n",
    "                        mult_factor=config.mult_factor\n",
    "                    )\n",
    "                ])\n",
    "            )\n",
    "\n",
    "        self.latent_final_norm = nn.LayerNorm(latent_dim) if final_norm else nn.Identity()\n",
    "        self.latents_to_interface_cross_attn = Attention(\n",
    "            output_dim=interface_dim,\n",
    "            context_dim=latent_dim,\n",
    "            **attn_kwargs\n",
    "        )\n",
    "        self.interface_ffn = FeedForwardLayer(\n",
    "            input_dim=interface_dim,\n",
    "            time_dim=time_dim,\n",
    "            mult_factor=config.mult_factor\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        interface,\n",
    "        latents,\n",
    "        time\n",
    "    ):\n",
    "        latents = self.interface_to_latents_cross_attn(latents, interface, time) + latents\n",
    "        latents = self.latents_ffn(latents, time) + latents\n",
    "        \n",
    "        for attn, ffn in self.latent_self_attn_layers:\n",
    "            latents = attn(latents, latents, time=time) + latents\n",
    "            latents = ffn(latents, time) + latents\n",
    "        \n",
    "        interface = self.latents_to_interface_cross_attn(interface, latents, time) + interface\n",
    "        interface = self.interface_ffn(interface, time) + interface\n",
    "        \n",
    "        latents = self.latent_final_norm(latents)\n",
    "        \n",
    "        return interface, latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec6ea1da-2596-4f96-8262-48ae50b88670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        head_dim,\n",
    "        time_dim,\n",
    "        attn_dropout,\n",
    "        mult_factor,\n",
    "        input_norm,\n",
    "        context_norm,\n",
    "        qk_norm\n",
    "    ):\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.mult_factor = mult_factor\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.input_norm = input_norm\n",
    "        self.context_norm = context_norm\n",
    "        self.qk_norm = qk_norm\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7608519d-3fa1-412a-9551-834e94650559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rin_block_module():\n",
    "    config = Config(4,32,512,0.4,4,True,True,True)   \n",
    "    rinblock = RINBlock(config, 256, 4, 512, 512, True)\n",
    "    interface = torch.randn(32, 256, 256)\n",
    "    latents = torch.randn(32, 128, 512)\n",
    "    time = torch.randn(32, 512)\n",
    "    interface, latents = rinblock(interface, latents, time)\n",
    "    assert interface.shape == torch.randn(32, 256, 256).shape\n",
    "    assert latents.shape == torch.randn(32, 128, 512).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91b6a055-c928-4086-b0e4-fb594e037ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of code related to patches, we need to have something related to text.\n",
    "# the process of converting image to patches and then to patch embedding needs to be replaced\n",
    "# by something like word embeddings. \n",
    "# This could be byte embeddings or word embeddings (sub-word) to begin with.\n",
    "class RIN(nn.Module): ### WIP ###\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim:int,\n",
    "        num_rin_blocks:int,\n",
    "        num_latent_attn_layers:int,\n",
    "        vocab_size:int,\n",
    "        interface_emb_dim:int, # embedding size of words in interface\n",
    "        latent_dim:int,\n",
    "        num_latents:int,\n",
    "        learned_sinusoidal_dim:int, # TODO: check if needed\n",
    "        latent_token_time_cond:bool,        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latent_token_time_cond = latent_token_time_cond\n",
    "        self.interface_embedding = nn.Embedding(vocab_size, interface_emb_dim)\n",
    "\n",
    "        sinusoidal_pos_embedding = LearnedSinusoidalPosEmb(learned_sinusoidal_dim)\n",
    "        time_dim = hidden_dim*4\n",
    "        fourier_dim = learned_sinusoidal_dim + 1\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            sinusoidal_pos_embedding,\n",
    "            nn.Linear(fourier_dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
    "        nn.init.normal_(self.latents, std=0.02)\n",
    "        self.initial_latents = nn.Sequential(\n",
    "            FeedForwardLayer(latent_dim),\n",
    "            nn.LayerNorm(latent_dim)\n",
    "        )\n",
    "        nn.init.zeros_(self.initial_latents[-1].gamma)\n",
    "\n",
    "        self.rin_blocks = nn.ModuleList([\n",
    "            RINBlock(\n",
    "                config,\n",
    "                hidden_dim,\n",
    "                num_latent_attn_layers,\n",
    "                latent_dim,\n",
    "                time_dim,\n",
    "                config.final_norm\n",
    "            ) for _ in range(num_rin_blocks)\n",
    "        ])\n",
    "        self.unembed = nn.Linear(hidden_dim, vocab_size)\n",
    "        # TODO: check if this should be tied to embedding weight\n",
    "\n",
    "    def forward(self, input_tokens, time, input_self_cond, latent_self_cond, return_latents):\n",
    "        # check if input_self_cond (input self conditioning - at interface) is done. \n",
    "        # I only remember it being done for latents. Not adding code for that right now.\n",
    "\n",
    "        batch_size = input_tokens.shape[0]\n",
    "        time_emb = self.time_mlp(time)\n",
    "\n",
    "        latents = repeat(self.latents, 'n d -> b n d', b=batch_size)\n",
    "        if latent_self_cond is not None:\n",
    "            latents = latents + self.initial_latents(latent_self_cond)\n",
    "\n",
    "        # TODO: not sure about the theory behind this. Just copied for now. Check if needed.\n",
    "        if self.latent_token_time_cond:\n",
    "            # whether the time conditioning is to be treated as one latent token or for \n",
    "            # projecting into scale and shift for adaptive layernorm\n",
    "            time_emb = rearrange(time_emb, 'b d -> b 1 d')\n",
    "            latents = torch.cat([latents, time_emb], dim=-2)\n",
    "\n",
    "        interface_emb = self.interface_embedding(input_tokens)\n",
    "        # TODO: add positional embedding for tokens\n",
    "\n",
    "        for rin_block in self.rin_blocks:\n",
    "            interface, latents = rin_block(interface_emb, latents, time_emb)\n",
    "\n",
    "        out = self.unembed(interface)\n",
    "        # need to take softmax to get the predicted tokens\n",
    "\n",
    "        return out, latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a3c42-3b1e-47ee-9e24-b98b7d7fbe37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
